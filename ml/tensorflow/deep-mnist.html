

<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <title>深入探索MNIST — Itimor's Book</title>
    <meta charset="utf-8">
    <meta name="description" content="Itimor's Book">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

    <meta property="og:type" content="article">
    <meta property="og:title" content="深入探索MNIST — Itimor's Book">
    <meta property="og:description" content="Itimor's Book">
    <meta property="og:image" content="https://itimor.github.io//images/logo.png">

    <link rel="icon" type="image/png" sizes="32x32" href="/images/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="96x96" href="/images/icons/favicon-96x96.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/icons/favicon-16x16.png">
    <link rel="icon" href="/images/logo.png" type="image/png">
      
    <meta name="baidu-site-verification" content="ah99c7kYSG" />
    <meta name="msapplication-TileColor" content="#4fc08d">
    <meta name="theme-color" content="#4fc08d">
    <meta name="msapplication-config" content="browserconfig.xml">


    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- main page styles -->
    <link rel="stylesheet" href="/css/page.css">

    <!-- this needs to be loaded before guide's inline scripts -->
    <script>window.PAGE_TYPE = "tensorflow"</script>

    <!-- ga -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', '', 'itimor.github.io');
      ga('send', 'pageview');
    </script>
  </head>
  <body class="docs">
    <div id="mobile-bar" >
      <a class="menu-button"></a>
      <a class="logo" href="/"></a>
    </div>
    <div id="header">
  <a id="logo" href="/">
   <!-- <img src="/images/logo.png"> -->
    <span>Itimor's Book</span>
  </a>
  <ul id="nav">
    <li>
  <form id="search-form">
    <input type="text" id="local-search-input" class="local-search-input">
    <div id="local-search-result" class="local-search-result"></div>
  </form>
</li>
<li class="nav-dropdown-container learn">
  <a class="nav-link">机器学习</a><span class="arrow"></span>
  <ul class="nav-dropdown">
    <li><ul>
      <li><a href="/ml/tensorflow/" class="nav-link current">Tensorflow</a></li>
      <li><a href="/ml/mllib/" class="nav-link">基础包介绍</a></li>
    </ul></li>
  </ul>
</li>

<li>
  <a href="/about/" class="nav-link team">about</a>
</li>
  </ul>
</div>
    
      <div id="main" class="fix-sidebar">
        
          
  <div class="sidebar">
  <div class="sidebar-inner">
    <ul class="main-menu">
      <li>
  <form id="search-form">
    <input type="text" id="local-search-input" class="local-search-input">
    <div id="local-search-result" class="local-search-result"></div>
  </form>
</li>
<li class="nav-dropdown-container learn">
  <a class="nav-link">机器学习</a><span class="arrow"></span>
  <ul class="nav-dropdown">
    <li><ul>
      <li><a href="/ml/tensorflow/" class="nav-link current">Tensorflow</a></li>
      <li><a href="/ml/mllib/" class="nav-link">基础包介绍</a></li>
    </ul></li>
  </ul>
</li>

<li>
  <a href="/about/" class="nav-link team">about</a>
</li>
    </ul>
    <div class="list">
      <h2>
        
        Tensorflow教程
      </h2>
      <ul class="menu-root">
  
    
    
    
    <li>
      <a href="/ml/tensorflow/index.html" class="sidebar-link">Tensorflow入门</a>
    </li>
  
    
    
    
    <li>
      <a href="/ml/tensorflow/mnist.html" class="sidebar-link">MNIST机器学习入门</a>
    </li>
  
    
    
    
    <li>
      <a href="/ml/tensorflow/deep-mnist.html" class="sidebar-link current">深入探索MNIST</a>
    </li>
  
    
    
    
    <li>
      <a href="/ml/tensorflow/tensorflow101.html" class="sidebar-link">TensorFlow运作方式入门</a>
    </li>
  
  
</ul>

    </div>
  </div>
</div>


<div class="content tensorflow with-sidebar ">
  
    
  
  
    <h1>深入探索MNIST</h1>
  
  <p>TensorFlow是一个非常强大的用来做大规模数值计算的库。其所擅长的任务之一就是实现以及训练深度神经网络。</p>
<p>在本教程中，我们将学到构建一个TensorFlow模型的基本步骤，并将通过这些步骤为MNIST构建一个深度卷积神经网络。</p>
<p>本介绍假定熟悉神经网络和 MNIST 数据集。如果你没有他们的背景, 看看<a href="/ml/tensorflow/">新手指南</a>。请务必在启动前安装 TensorFlow。</p>
<h2 id="关于本教程"><a href="#关于本教程" class="headerlink" title="关于本教程"></a>关于本教程</h2><p>本教程的第一部分解释了 <a href="https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py" target="_blank" rel="noopener">mnist_softmax.py</a>, 这是 Tensorflow 模型的基本实现。第二部分介绍了提高测量精度的方法。</p>
<p>您可以将本教程中的每个代码段复制并粘贴到 Python 环境中, 或者您可以从 <a href="https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_deep.py" target="_blank" rel="noopener">mnist_deep.py</a> 下载完全实现的代码。</p>
<p>我们将在本教程中完成的任务：</p>
<ul>
<li>创建一个softmax回归函数，该函数是基于查看图像中的每个像素来识别mnist数据的模型</li>
<li>使用 Tensorflow 来训练模型, 通过让它 “看” 成千上万的例子来识别数字 (并运行我们的第一个 Tensorflow 会话</li>
<li>用我们的测试数据检查模型的准确性</li>
<li>构建、训练和测试多层卷积神经网络以提高结果</li>
</ul>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>在创建模型之前，我们会先加载MNIST数据集，然后启动一个TensorFlow的session。</p>
<h3 id="加载MNIST数据"><a href="#加载MNIST数据" class="headerlink" title="加载MNIST数据"></a>加载MNIST数据</h3><p>为了方便起见，我们已经准备了<a href="https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/tutorials/mnist/input_data.py" target="_blank" rel="noopener">一个脚本</a>来自动下载和导入MNIST数据集。它会自动创建一个 <code>&#39;MNIST_data&#39;</code> 的目录来存储数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>这里，<code>mnist</code> 是一个轻量级的类。它以Numpy数组的形式存储着训练、校验和测试数据集。同时提供了一个函数，用于在迭代中获得 <code>minibatch</code> ，后面我们将会用到。</p>
<h3 id="运行InteractiveSession"><a href="#运行InteractiveSession" class="headerlink" title="运行InteractiveSession"></a>运行InteractiveSession</h3><p>Tensorflow依赖于一个高效的C++后端来进行计算。与后端的这个连接叫做session。一般而言，使用TensorFlow程序的流程是先创建一个图，然后在session中启动它。</p>
<p>这里，我们使用更加方便的 <code>InteractiveSession</code> 类。通过它，你可以更加灵活地构建你的代码。它能让你在运行图的时候，插入一些<a href="https://www.tensorflow.org/versions/master/get_started/get_started#the_computational_graph" target="_blank" rel="noopener">计算图</a>，这些计算图是由某些操作(operations)构成的。这对于工作在交互式环境中的人们来说非常便利，比如使用IPython。如果你没有使用 <code>InteractiveSession</code>，那么你需要在启动session之前构建整个计算图，然后<a href="https://www.tensorflow.org/versions/master/get_started/get_started#the_computational_graph" target="_blank" rel="noopener">启动该计算图</a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br></pre></td></tr></table></figure>
<h3 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h3><p>为了在 python 中进行高效的数值计算, 我们通常使用像 NumPy 这样的库, 它使用其他语言实现的高效代码, 例如在 python 之外执行昂贵的运算 (如矩阵乘法)。不幸的是, 在每个操作中切换回 Python 仍然会有很多开销。如果要在 gpu 或分布式方式下运行计算, 而传输数据的成本很高, 这一开销更加可怖。</p>
<p>TensorFlow也是在Python外部完成其主要工作，但是进行了改进以避免这种开销。其并没有采用在Python外部独立运行某个耗时操作的方式，而是先让我们描述一个交互操作图，然后完全将其运行在Python外部。这与Theano或Torch的做法类似。</p>
<p>因此Python代码的目的是用来构建这个可以在外部运行的计算图，以及安排计算图的哪一部分应该被运行。详情请查看<a href="/ml/tensorflow/">tensroflow入门</a>中的计算图表一节。</p>
<h2 id="构建Softmax-回归模型"><a href="#构建Softmax-回归模型" class="headerlink" title="构建Softmax 回归模型"></a>构建Softmax 回归模型</h2><p>在这一节中我们将建立一个拥有一个线性层的softmax回归模型。在下一节，我们会将其扩展为一个拥有多层卷积网络的softmax回归模型。</p>
<h3 id="占位符"><a href="#占位符" class="headerlink" title="占位符"></a>占位符</h3><p>我们通过为输入图像和目标输出类别创建节点，来开始构建计算图。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>这里的<code>x</code>和<code>y</code>并不是特定的值，相反，他们都只是一个<code>占位符</code>，可以在TensorFlow运行某一计算时根据该占位符输入具体的值。</p>
<p>输入图片x是一个2维的浮点数张量。这里，分配给它的<code>shape</code>为<code>[None, 784]</code>，其中<code>784</code>是一张展平的MNIST图片的维度。<code>None</code>表示其值大小不定，在这里作为第一个维度值，用以指代batch的大小，意即<code>x</code>的数量不定。输出类别值<code>y_</code>也是一个2维张量，其中每一行为一个10维的one-hot向量,用于代表对应某一MNIST图片的类别。</p>
<p>虽然<code>placeholder</code>的<code>shape</code>参数是可选的，但有了它，TensorFlow能够自动捕捉因数据维度不一致导致的错误。</p>
<h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p>我们现在为模型定义权重W和偏置b。可以将它们当作额外的输入量，但是TensorFlow有一个更好的处理方式：变量。一个变量代表着TensorFlow计算图中的一个值，能够在计算过程中使用，甚至进行修改。在机器学习的应用过程中，模型参数一般用Variable来表示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br></pre></td></tr></table></figure>
<p>我们在调用<code>tf.Variable</code>的时候传入初始值。在这个例子里，我们把<code>W</code>和<code>b</code>都初始化为零向量。<code>W</code>是一个784x10的矩阵（因为我们有784个特征和10个输出值）。b是一个10维的向量（因为我们有10个分类）。</p>
<p><code>变量</code>需要通过seesion初始化后，才能在session中使用。这一初始化步骤为，为初始值指定具体值（本例当中是全为零），并将其分配给每个<code>变量</code>,可以一次性为所有<code>变量</code>完成此操作。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure>
<h3 id="类别预测与损失函数"><a href="#类别预测与损失函数" class="headerlink" title="类别预测与损失函数"></a>类别预测与损失函数</h3><p>现在我们可以实现我们的回归模型了。这只需要一行！我们把向量化后的图片x和权重矩阵W相乘，加上偏置b，然后计算每个分类的softmax概率值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = tf.matmul(x,W) + b</span><br></pre></td></tr></table></figure>
<p>我们可以简单地指定一个损失函数。损失表明模型的预测在一个单一的例子是多么糟糕;我们试图尽量减少, 而在所有的例子培训。在这里, 我们的损失函数是熵的目标和 softmax 激活函数应用于模型的预测。在初学者教程中, 我们使用稳定的公式:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cross_entropy = tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))</span><br></pre></td></tr></table></figure>
<p>请注意, <code>tf.nn.softmax_cross_entropy_with_logits</code> 内部将 softmax 应用于模型的规范化模型预测和所有类的求和, 而 <code>tf. reduce_mean</code> 的平均值超过这些总和。</p>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>我们已经定义好模型和训练用的损失函数，那么用TensorFlow进行训练就很简单了。因为TensorFlow知道整个计算图，它可以使用自动微分法找到对于各个变量的损失的梯度值。TensorFlow有<a href="https://www.tensorflow.org/versions/master/api_guides/python/train#optimizers" target="_blank" rel="noopener">大量内置的优化算法</a> 这个例子中，我们用最速下降法让交叉熵下降，步长为0.5.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>
<p>这一行代码实际上是用来往计算图上添加一个新操作，其中包括计算梯度，计算每个参数的步长变化，并且计算出新的参数值。</p>
<p>返回的<code>train_step</code>操作对象，在运行时会使用梯度下降来更新参数。因此，整个模型的训练可以通过反复地运行<code>train_step</code>来完成。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">  batch = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">  train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>]&#125;)</span><br></pre></td></tr></table></figure>
<p>每一步迭代，我们都会加载100个训练样本，然后执行一次<code>train_step</code>，并通过<code>feed_dict</code>将<code>x</code> 和 <code>y_</code>张量<code>占位符</code>用训练训练数据替代。</p>
<p>注意，在计算图中，你可以用<code>feed_dict</code>来替代任何张量，并不仅限于替换占位符。</p>
<h3 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h3><p>那么我们的模型性能如何呢？</p>
<p>首先让我们找出那些预测正确的标签。<code>tf.argmax</code> 是一个非常有用的函数，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。由于标签向量是由0,1组成，因此最大值1所在的索引位置就是类别标签，比如<code>tf.argmax(y,1)</code>返回的是模型对于任一输入x预测到的标签值，而 <code>tf.argmax(y_,1)</code> 代表正确的标签，我们可以用 <code>tf.equal</code> 来检测我们的预测是否真实标签匹配(索引位置一样表示匹配)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>这里返回一个布尔数组。为了计算我们分类的准确率，我们将布尔值转换为浮点数来代表对、错，然后取平均值。例如：<code>[True, False, True, True]</code>变为<code>[1,0,1,1]</code>，计算出平均值为0.75。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br></pre></td></tr></table></figure>
<p>最后，我们可以计算出在测试数据上的准确率，大概是92%。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(accuracy.eval(feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure>
<h2 id="构建一个多层卷积网络"><a href="#构建一个多层卷积网络" class="headerlink" title="构建一个多层卷积网络"></a>构建一个多层卷积网络</h2><p>在MNIST上只有92%正确率，实在太糟糕。在这个小节里，我们用一个稍微复杂的模型：卷积神经网络来改善效果。这会达到大概99.2%的准确率。虽然不是最高，但是还是比较让人满意。</p>
<p>这里是一个用张量板创建的关于我们将建立的模型的图表:</p>
<p><img src="images/6093ed9c.png" alt=""></p>
<h3 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h3><p>为了创建这个模型，我们需要创建大量的权重和偏置项。这个模型中的权重在初始化时应该加入少量的噪声来打破对称性以及避免0梯度。由于我们使用的是ReLU神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为0的问题（dead neurons）。为了不在建立模型的时候反复做初始化操作，我们定义两个函数用于初始化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">  initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">  <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">  initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">  <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure>
<h3 id="卷积和池化"><a href="#卷积和池化" class="headerlink" title="卷积和池化"></a>卷积和池化</h3><p>TensorFlow在卷积和池化上有很强的灵活性。我们怎么处理边界？步长应该设多大？在这个实例里，我们会一直使用vanilla版本。我们的卷积使用1步长（stride size），0边距（padding size）的模板，保证输出和输入是同一个大小。我们的池化用简单传统的2x2大小的模板做max pooling。为了代码更简洁，我们把这部分抽象成一个函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="第一层卷积"><a href="#第一层卷积" class="headerlink" title="第一层卷积"></a>第一层卷积</h3><p>现在我们可以开始实现第一层了。它由一个卷积接一个max pooling完成。卷积在每个5x5的patch中算出32个特征。卷积的权重张量形状是<code>[5, 5, 1, 32]</code>，前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。 而对于每一个输出通道都有一个对应的偏置量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br></pre></td></tr></table></figure>
<p>为了用这一层，我们把x变成一个4d向量，其第2、第3维对应图片的宽、高，最后一维代表图片的颜色通道数(因为是灰度图所以这里的通道数为1，如果是rgb彩色图，则为3)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>我们把<code>x_image</code>和权值向量进行卷积，加上偏置项，然后应用ReLU激活函数，最后进行max pool。<code>max_pool_2x2</code> 方法将图像大小缩小为14x14。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)</span><br></pre></td></tr></table></figure>
<h3 id="第二层卷积"><a href="#第二层卷积" class="headerlink" title="第二层卷积"></a>第二层卷积</h3><p>为了构建一个更深的网络，我们会把几个类似的层堆叠起来。第二层中，每个5x5的patch会得到64个特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line"></span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)</span><br></pre></td></tr></table></figure>
<h3 id="密集连接层"><a href="#密集连接层" class="headerlink" title="密集连接层"></a>密集连接层</h3><p>现在，图片尺寸减小到7x7，我们加入一个有1024个神经元的全连接层，用于处理整个图片。我们把池化层输出的张量reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line"></span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br></pre></td></tr></table></figure>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>为了减少过拟合，我们在输出层之前加入dropout。我们用一个placeholder来代表一个神经元的输出在dropout中保持不变的概率。这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。 TensorFlow的tf.nn.dropout操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br></pre></td></tr></table></figure>
<h3 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h3><p>最后，我们添加一个softmax层，就像前面的单层softmax regression一样。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</span><br></pre></td></tr></table></figure>
<h3 id="训练和评估模型"><a href="#训练和评估模型" class="headerlink" title="训练和评估模型"></a>训练和评估模型</h3><p>这个模型的效果如何呢？为了进行训练和评估，我们使用与之前简单的单层SoftMax神经网络模型几乎相同的一套代码。</p>
<p>不同的是：</p>
<ul>
<li>我们将取代最陡峭的梯度下降优化器与更复杂的 ADAM优化器。</li>
<li>在feed_dict中加入额外的参数keep_prob来控制dropout比例</li>
<li>每100次迭代输出一次日志。</li>
</ul>
<p>我们也将使用 <code>tf.Session</code> 而不是 <code>tf.InteractiveSession</code>, 这更好地区分了创建图形 (模型规范) 的过程和计算图 (模型拟合) 的过程。它通常会使代码更简洁。<code>tf.Session</code> 在 <a href="https://docs.python.org/3/whatsnew/2.6.html#pep-343-the-with-statement" target="_blank" rel="noopener">with block</a> 中创建, 以便在块退出后自动销毁。</p>
<p>随时运行此代码。请注意, 它会进行2万次训练迭代, 并且可能需要一段时间 (可能长达半小时), 这取决于您的处理器。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))</span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(tf.global_variables_initializer())</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">    batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">      train_accuracy = accuracy.eval(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">      print(<span class="string">'step %d, training accuracy %g'</span> % (i, train_accuracy))</span><br><span class="line">    train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line"></span><br><span class="line">  print(<span class="string">'test accuracy %g'</span> % accuracy.eval(feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure>
<p>以上代码，在最终测试集上的准确率大概是99.2%。</p>
<p>目前为止，我们已经学会了用TensorFlow快捷地搭建、训练和评估一个复杂一点儿的深度学习模型。</p>

  
</div>

        
      </div>
      <script src="/js/smooth-scroll.min.js"></script>
    

    <!-- main custom script for sidebars, version selects etc. -->
    <script src="/js/css.escape.js"></script>
    <script src="/js/common.js"></script>
    <script src="/js/local_search.js"></script>

    <!-- search -->
    <script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
    <script>
    var path = "/search.xml";
    searchFunc(path, 'local-search-input', 'local-search-result');
    var inputArea       = document.querySelector("#local-search-input");
    inputArea.onclick   = function(){ getSearchFile(); this.onclick = null }
    inputArea.onkeydown = function(){ if(event.keyCode == 13) return false }
    var $resultContent = document.getElementById('local-search-result');
    var BTN = "<i id='local-search-close'>×</i>";
    $resultContent.innerHTML = BTN + "<ul><span class='local-search-empty'>Please wait for 1024 seconds ……<span></ul>";
    </script>

    <!-- fastclick -->
    <script src="//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js"></script>
    <script>
    document.addEventListener('DOMContentLoaded', function() {
      FastClick.attach(document.body)
    }, false)
    </script>
  </body>
</html>
